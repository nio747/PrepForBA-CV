[TOC]

# 基础理论

### 结构：

ConvNet: **hierarchical** model. raw data --> convolution, pooling, non-linear activation function (**feed-forward**) --> objective function, **loss** --> **back**-propagation --> parameter **update** --> feed-forward again --> ...**loop** --> the model **converges**.

 **feed-forward**: 如cross-entropy loss function: **x**^L^ 的每一维表示**x**^1^分别隶属于C个类别的后验概率。--> 预测标记。

**Back-forward**: non-convex! --> 随机梯度下降法 Stochastic Gradient Descent, SGD 和 误差反向传播 error back propagation.  

大规模应用：**mini-batch SGD**：训练阶段随机抽取n个样本作为一批batch样本，进行feed-forward，prediction(x^L^)，loss，SGD，update从后逐层向前知道第一层（此个参数更新过程为mini-batch）。不同batch按无放回抽样遍历所有训练集样本，遍历完全称为一轮（epoch）。batch_size不宜过小，否则更新参数不一定为全局最优（仅为局部最优），--> 振荡。 一般为32, 64, 128, 256.

**参数更新**：利用参数w^i^的导数对其进行更新，需依赖对x的偏导数+链式函数。

```
for epoch = 1..T ：
	while 未遍历完全：
		前馈得每层x_i,计算最终loss；
		for i=L..1 (L层)：
			反向计算loss对该层参数w的偏导数；
			反向计算loss对该层输入x的偏导数；
			更新w_i = w_i - step * dw_i
		end for
	end while
end for
return w_i
```



# 基本部件

## 端到端 End-to-end manner

深度学习的一个重要思想。属于representation learning的一种。DL之前，样本表示基本要使用**hand-crafted feature**, 其优劣决定了最终的精度，所谓feature engineering。DL：另一种范式/paradigm，不进行人为的子问题划分（global/local descriptor...），而是完全交给模型直接学习从原始输入到期望输出的映射。

输入raw data --> 众多操作层($f_{CNN}$) --> loss = data loss + regularization loss --> backprop, update --> loop --> .. 

中间的操作层起到了将raw data映射为特征（特征学习），再映射为样本标记（如分类）。有卷积层、激活函数、池化、全连接层等等。

第l层输入为三维张量(i^l^, j^l^, d^l^)，d为通道。一般采用mini-batch，故事实上是一个四维张量，还包含了N（batch size）.

## 层。

#### 卷积层：

convolution kernel, convolution filter 5x5, 3x3 ..  filter中为weight，且同一个depth slice上有parameter sharing。通常还会加上bias term。重要的hyper parameters：filter size，stride。有各种可以检测不同pattern的卷积核（如边缘、横向、纵向滤波器等等），在网络中相互组合，将样本中一般模式抽象出来。

#### 汇合层：

不包含需要学习的参数（fixed-function），多为average-pooling，max-pooling。需指定类型，kernel size，stride等超参数。多为max，2x2，S=2. 也有stochastic-pooling（随机抽取，元素值大的activation被选中的概率也大，全局意义上和平均值汇合近似，局部上和最大值汇合近似）。

汇合为一种down-sampling操作，也可看作是一个用p-norm作为非线性映射的卷积操作，p趋近正无穷时就是max-pooling。功效：

- feature invariant 特征不变性。使模型更关注是否存在某些特征而不是其具体的位置。包含某种程度自由度，容忍一些特征微小的位移。
- 特征降维。pooling结果的一个元素对应原输入的sub-region。~spatially dimension reduction，使其能抽取更广范围的特征，且减小计算量和参数两。
- 防止overfitting。
- 也有：使用stride convolutional layer代替pooling实现将采样--all conv layers。outperformance

#### Activation function

又称non-linearity mapping。增加网络的表达能力（非线性，否则仅为单纯的线性映射，不够复杂；直观上也模拟了生物神经元，超过阈值则激活，否则抑制——sigmoid）

**Sigmoid**: not zero-centered, saturation effect-->kill gradient，no longer update。

**ReLU**，正值部分消除梯度饱和，收敛速度快。首选。

#### FC layer

起分类器作用。前面层是将raw data映射到隐层特征空间。FC则将学到的特征映射到样本的标记空间。FC可由卷积操作实现：FC后的FC：1x1卷积；Conv后的FC：h x w的全局卷积，h，w为前层卷积输出的高和宽。如VGG最后conv输出7x7x512，后层为4096神经元的FC，则可用7x7x512x4096的全局卷积实现，得1x1x4096的输出。

#### 目标函数

衡量前馈输出的预测值和真实的样本标记之间的误差，最常用交叉熵（分类）和l2损失（回归）。



# 经典结构

## 重要概念

**receptive field 感受野**：随网络深度加深，后层神经元在第一层输入层的感受野会随之增大。如单层7x7卷积相当于多层卷积L+3层3x3卷积在L层对应的感受野。小卷积核通过多层叠加可以取得和大核相同的感受野。用小核+深网=增加model capacity和model complexity，且同时减少了参数个数。

另外还有dilated convolution和deformable convolutional networks等。

**distributed representation 分布式表示**

传统representation learning：bag-of-word model。图像局部特征=visual word，所有图像的局部特征=vocabulary。一张图可以用他的单词来表示，且这些单词通过词典映射成一条representation vector（这是离散式表示，每个维度对应一个明确的pattern和concept）。

深度CNN中为分布式表示：concept到neuron是一个多对多映射——每个语义概念由许多分布在不同neuron中被激活的pattern表示；而每个neuron参加许多语义概念的表示。

且neuron响应的区域多呈现稀疏（sparse）特性，即响应区域集中且占原图比例较小。

**深度特征的层次性**：

各层得到的深度特征逐渐从泛化特征（边缘，纹理）过渡到高层语义表示（躯干、头部）。得益于此，不同层特征可以信息互补，因此对单个模型“多层特征融合”（multi-layer ensemble）为很直接有效的网络集成技术，提高精度。。

## 案例分析

CV顶会：CVPR，ICCV，ECCV。顶刊：IEEE TPAMI，IJCV，IEEE TIP

#### Alex-Net

2012年；Alex Krizhevsky。五conv三FC。两GPU并行（两支一样的网络）。和LeNet思路差不多，更深更复杂。ImageNet（海量数据）。训练技巧：ReLU，局部响应规范化，防止过拟合的data augmentation（数据增广）和dropout（随即失活）。

**LRN 局部响应规范化**。对相同空间位置上相邻深度（adjacent depth）的卷积结果做规范化。LRN的一种特殊情况——L2规范化。

#### VGG-Nets

2014；良好的泛化性能，其在imagenet上的pre-trained model常用于（除了feature extractor以外）诸多问题：object proposal, fine-grained object localization and image retrieval, co-localization.

普遍适用了小卷积核和“保持大小”，确保输入随深度不剧烈减少，通道数随层数逐渐增加。

#### Network-In-Network

用多层感知机（多层FC和NL的组合）代替了简单的线性卷积层，复杂度更高，提供了网络层间映射的新可能，也增加了非线性能力。此想法被residual network和Inception等借鉴。

另一重大突破：摒弃FC作分类层，而是用global average pooling。最后一层有C张feature map对应C类别，pooling分别作用于各项特征图，最后以pooling结果映射到样本标记。

#### Residual Network

NN的depth和width是复杂度的两个核心，不过depth更有效。然而深度加深=训练更困难（SGD的训练中，误差信号的多层backprop很容易引发梯度弥散，即梯度过小使回传的训练误差及其微弱；或爆炸，即梯度过大-->NaN；可通过特殊的weight initialization和batch Normalization改善），且常规网络深度加深不等于效果变好！

先行代表：**highway network 高速公路网络**

常规网络一层的输出：$y=\mathcal F(x, w_f)$.  F-非线性激活函数。

高速公路：$y = \mathcal F(x, w_f) · \mathcal T(x, w_t) + x· \mathcal C(x, w_c)$ . T 和 C 是两个非线性变换，称为变换门gate（控制变换强度） 和携带门（控制元信号的保留强度）。简化：$\mathcal C = 1- \mathcal T$. 当T为恒等映射时，退化成常规网络。

主角：**深度残差网络**，2015

可以看作是highway的一种特殊情况: T和C都是恒等变化——

$y = \mathcal F(x, w) + x$ --> $\mathcal F(x,w) = y - x $. 即需要学习的函数F其实为残差项y-x，故称之为残差函数。

残差网络由多个**残差模块**堆叠而成。其中有bottleneck residual block（瓶颈残差模块）：3个1x1，3x3，1x1卷积层构成，1x1起降维或升维作用。使3x3可以在相对较低维度的输入上进行，提高计算效率。

residual block中的近路连接short cut可通过简单的恒等映射，而不用复杂的T和C。可以对特别深的网络进行back prop端到端，使用简单的SGD就能训练。若无short cut，其实就是把fc换成global average pooling layer（GoogleLeNet在15年就用了这个理念）的VGG Nets。减少了参数和过拟合。

各网络见该章附表中详细数据。



# ConvNet的压缩

识别一张图片用的参数太多，占用空间太大，运算太多，很难在移动端运用；且有严峻的过参数化问题（over-parameterization）——模型内部参数巨大冗余！

压缩可行，不过冗余在模型训练阶段十分必要（SGD的非凸优化，冗余参数保证了收敛到比较好的最优值；且一定程度上深网多参复杂模型=更好结果。）

现行：多为将庞大的pre-trained model转化为精简的小模型。/设计更紧凑的网络结构，通过对新的小模型来训练获得精简模型。

<u>前端压缩</u>：不改变结构（仅减少层数或filter个数，最终模型仍可完美适配现有DL库如Caffe），知识蒸馏，紧凑结构，filter层面的剪枝等；

<u>后端压缩</u>：低秩近似，不限制剪枝，参数量化，二值网络：尽可能减少模型大小，对原有网络极大改造（往往不可逆），必须开发相配的运行库/硬件设备，巨大维护成本。

可以相互结合。

#### 低秩近似

将稠密的权重矩阵由若干个小规模矩阵近似重构出来，降低存储和计算开销。

1. 使用结构化矩阵（一系列拥有特殊结构的矩阵，如Toeplitz矩阵）进行低秩分解。可以使用快速傅里叶变换。小数据集上2-3倍压缩，最终精度甚至更好。
2. 直接使用矩阵分解来降低W矩阵的参数。如奇异值分解（singular Value Decomposition，SVD）W=USV。S为奇异值矩阵，保留前k个最大项……etc. 卷积层压缩2-3倍，FC压缩5-13倍，速度两倍提升。精度损失1%以内。

在中小型网络不错，但超参数量随层数现行变化，搜索空间急剧增大。在大型模型是否可用有待商榷。

#### 剪枝，稀疏约束

剪枝：如：C4.5决策树算法。神经网络初始化训练，需要一定冗余来保证可塑性和capacity，完成后可剪枝。算法多基于二阶梯度计算w的重要程度，对小型网络尚可。

**基本框架**：1. 衡量neuron重要程度（根据剪枝粒度granularity不同，neuron可以是一个权重连接，也可以指整个滤波器）；2.移除一部分不重要的神经元（阈值判断或剪掉一定比例，后者更渐变灵活）；3. 网络微调（剪枝会影响精度，对剪枝后的模型进行微调。对大型数据集，微调会占用大量计算资源。需要调到什么程度？）4. 返回第一步，进行下一轮剪枝。

Han：首先将**低于阈值**的权重连接全部剪除，再对网络进行微调，反复迭代，至性能和规模较好的平衡。还可借助l1，l2正则化使权重趋向于零。不足: 剪枝后网络非结构化，连接在分布上没有连续性（随机稀疏结构）；约束了实际加速效果，依赖专门的运行库，制约了模型的通用性。（粒度太细）

或提升剪枝粒度至**滤波器级别**（直接丢弃整个滤波器）；速度大小提升，通用性也不会受到影响。如何衡量kernel重要性——如基于kernel权重本身的统计量（如每个kernel的l1，l2值）。然而小权重对loss可能有很大影响，压缩率太大时，丢弃这些权重破坏严重。（粒度太粗放）

因此——**数据驱动**的剪枝：如根据输出中每一个channel的稀疏度判断相应滤波器的重要程度（如果某一kernel输出几乎为0，那可能冗余); Molchanov：计算每一个kernel对loss的影响程度（taylor）；

**稀疏约束**：在网络优化目标中加入权重的稀疏正则项，训练时部分权重趋于0，再把这些0值剪枝。~动态剪枝，只需一次训练，而非剪枝的循环操作。另外还有结构化稀疏约束。

应用：选择什么权重选择策略；对网络结构破坏程度极小，作为前端处理和后端压缩技术相结合。



#### 参数量化

（后端）。量化~权重中归纳出若干‘代表’，用来表示某一类权重的具体数值。‘代表’存于codebook中，原W矩阵只需记录‘代表’的索引。（ ~bag-of-words model）

**标量量化**（scalar quantization）：对每一个权重矩阵W，将其转化为向量**w**，对其进行k簇聚类（k-means clustering），获得码本codebook。再用回传梯度对码本进行更新。若压缩率较大，精度大幅下降。

结构化的向量量化方法——**乘积量化**（Product Quantization，PQ）：将向量空间划分为若干个不相交的子空间（W划分为s个子矩阵W^i^, 对其每一行进行聚类）再执行量化操作。-->QCNN. 

除了聚类，还可以用**哈希技术**，HashedNets。

本质思想：多个权重映射到同一数值--> 实现权重共享；不足：量化后网络是固定的，很难再做改变；通用性较差，要专门运行库。



#### 二值网络

量化方法的一种极端——参数只能是+1/-1; 借助位操作（逻辑门元件XNOR gate等）完成运算。单纯的二值网络很难训练。

**二值连接**（binary connect）=训练时：单精度（权重更新）+二值（前馈，反馈）；完成训练：权重二值化。

1. 如何对w进行二值化：根据正负（更实用）/随机。
2. 如何计算二值w的梯度：对符号函数进行放松。



#### 知识蒸馏 Knowledge distillation

迁移学习的一种，将大模型学到的知识迁移到精简小模型上，使其获得大模型相近的性能。何为‘知识’？如何‘蒸馏’？

Jimmy：Softmax输入包含更加丰富的监督信息——将小模型的训练转化为回归问题；效果有限；

Hinton：Softmax层输出包含了每个类别的预测概率——“软标签”。（还可使用超参数温度T控制预测概率的平滑温度）。T不易确定，而对小模型训练有较大影响；类别较多=软标签维度较高时，难以收敛。

#### 紧凑结构

SqueezeNet：使用了“Fire Module”基本单元；用1x1卷积对输入特征降维；紧凑输入特征，保证泛化；扩张。。

。。。



# 数据扩充 data augmentation

不是所有数据集都有海量样本。数据扩充--扩充样本量，增加样本多样性，可避免过拟合+模型性能提升。

**简单的方法**：horizontally flipping（扩充两倍）, random crops(一般作用于较大的正方形以避免矩形抠取后的拉伸操作，在随机位置取图像块image patch/crop, 每张图片抠取次数--扩充倍数)，scaling和rotating（增加网络对物体尺度/方向上的鲁棒性），color jittering（RGB/HSV/色调微调）...

特殊的方法——

**Fancy PCA** ：(AlexNet)先对所有训练数据的RGB作PCA（主成分分析），得到相应eigenvector $p_i$ 和eigenvalue $\lambda_i$, 由此得出一组随机值:$[p_1, p_2, p_3][\alpha_1\lambda_1,\alpha_2\lambda_2,\alpha_3\lambda_3]^T$. 将其作为扰动加入原像素值。其中$\alpha_i$取0为mean，0.1标准差的高斯分布的随机值。每一个epoch后，重新选取$\alpha_i$再重复上述操作。

> 可以近似捕获自然图像的重要特性——物体特质与光强/颜色变化无关。

**监督式数据扩充**：利用图像标记信息。不同于object-centric的分类任务，为scene-centric，依靠图像整体蕴含的high-level semantic；



# 数据预处理 Pre-processing

ML中对输入特征常做Normalization预处理。图像处理中：每个像素信息可以看作一种特征，实践中对每个特征减去平均值来中心化（中心式归一化mean normalization）；CNN中通常是：计算**训练集图像像素均值 mean image**，在处理训练/验证/测试集图像时按需分别减去该均值。（interpret~如可以一定程度移去背景信息，更突出物体信息）。



# 参数初始化

训练的expectation：合理的预处理+规范化，网络收敛后，参数理想状况基本保持正负各半，期望为0的状态。但若全零初始化——梯度更新一样，无法更新/训练。

**随机初始化**：随机参数服从Gaussian distribution或uniform distribution. 

如均值为0，方差为1的标准高斯分布standard normal distribution`w = 0.001 .* randn(n_in, n_out);`：问题：网络输出分布的方差会随神经元个数改变！——故在初始化时同时加上对方差大小的规范化，以保持方差的一致性（**Xavier method**）`w = (0.001 .* randn(n_in, n_out)) ./ sqrt(n);` n可能为n_in或(n_in+n_out)/2。收敛更快。但此方法未考虑非线性函数对输入的影响（用ReLU等函数时，输出数据期望往往不再为0）。——He：用`sqrt(n/2)`.

或均匀分布uniform：需指定期望区间。

使用**预训练模型 pre-trained model**的参数作为新任务上的参数初始化。

**推荐**： He方法，高斯分布或均匀分布的较小随机数，再对方差规范化；用pre-trained简便且有效。



# 激活函数

**Sigmoid**: not zero-centered, saturation effect, kill gradient.

**tanh** or hyperbolic tangent function: [-1,1], zero-centered, saturate, kill gradients.

### ReLU

max{0,x}. x>0: 无saturation； 计算也更简单，有助于SGD收敛。但是x<0时，梯度为0（对小于0的卷积结果进行响应后，无法再影响网络训练——死区）。

缓解死区——**leaky ReLU**: f(x) = $\alpha x$. alpha为0.01~0.001数量级的小正数。超参，设置比较难且敏感，因此不太稳定。

**参数化ReLU，PReLU**：将$\alpha$作为网络训练过程中一个可学习的变量。基于SGD等对其进行更新。还可以各通道独享（即各有一个）参数alpha（自由度更大，性能也更优）。但会增加过拟合风险。（另外可观察到：浅层网络的alpha远大于0，即浅层网络所需的非线性较弱。浅层特征多为边缘/纹理等特性的泛化特征。）

**随机化ReLU**：随机化超参alpha（训练时均匀分布，测试时指定均匀分布的分布期望(l+u)/2。）

**指数化线性单元 ELU**: Exponential Linear Unit. 计算量更大了。

实践中：多用RELU，但需注意模型参数初始化和learning rate的设置。为提高精度，可试试leaky，prelu，随机，elu等等。



# 目标函数 objective function

### 分类任务

样本真实标记实际对应一条one hot vector（真实类下为1，其他C-1维为0）；

**交叉熵 cross-entropy/ Softmax**：CNN中最常用。

**hinge loss**：SVM中最常用。比交叉熵略差。（错误越大的样本惩罚越重，但对噪声（比如标记出错/outlier样本点）抵抗力较差）

**坡道损失函数** ramp loss function 或 truncated hinge loss funtion

对噪声和离群点抗噪更好，鲁棒性更强。在误差较大区域‘s处’进行截断，使其不再大程度影响整个loss。

**大间隔交叉熵 large-margin softmax loss**：为使特征更具分辨能力要求二者差异更大，故引入m拉大差距——m=1是为传统softmax。~扩大类间距离，训练目标比传统交叉熵更困难——额外好处：防止过拟合。增大了不同类别间分类的置信度，提升特征分辨能力（discriminative ability）。

**中心损失函数 center loss function**：在考虑类间距离同时减小类内差异。其中有$c_{y_i}$为第y_i类所有深度特征的均值/中心。迫使所有y_i类的样本不要离中心太原，否则增大损失。应与其他类间距离配合使用：L_final = L_cross entropy + lambda*L_center. lambda越大类内差异占目标较大比重，反之亦然。比只用softmax更好，特别是人脸识别问题。



### 回归

而回归任务中样本真实标记每维都为实数，而非二值。所谓残差residual：预测值和真实标记的靠近程度。

l1或l2损失函数；tukey’s biweight（~ramp loss），鲁棒。



### 标记分布 label distribution

即非简单的离散标记，但也不同于回归问题的连续标记（回归问题中标记连续，但不符合一个合法的概率分布）。fuzzy。

如果h=（h1, ... hC) 为对于输入x_i的最终输出，再利用标记分布前，需将h（如利用softmax）转化为一个合法分布。

针对预测的标记向量y_dach，通常用Kullback-Leibler散度（KL divergence）来度量其于真实标记向量y的误差。KL loss。



**推荐**： cross-entropy最常用，优于hinge。large-margin和center loss有助于提高特征分辨力；ramp-loss是一类非凸损失函数，抗噪良好，用于噪声或离群点多的分类任务。一些年龄估计/头部角度识别等样本标记具有不确定性的场景下——基于标记分布的loss也是好选择。



# 正则化

泛化能力generalization ability, 在训练集表现优异的模型，在测试集上依然工作良好；否则overfitting。

防止overfitting——多用regularization（天空-->鸟笼）

### $l_2$正则化

常用l2或l1对操作层如conv/classification层进行正则化，约束模型复杂度：$l_2=\frac{1}{2} \lambda {||w||_2}^2$. $\lambda$为regularization strength，较大则约束更大；一般将正则项加入目标函数loss。又名weight decay, ridge regression.

l1: 还能起到使参数更稀疏的作用——稀疏化后使一部分参数为0，另一部分为非0实值，这部分起到选择重要参数/特征维度的作用，且可以去除噪声。

l1和l2联合使用——Elastic Net。

### 最大范数约束

max norm constraints：$||w||_2 < c$. c=10^3^, 10^4^.对参数规定了上限，即使学习率过大也不至于梯度爆炸。

### Dropout

训练时以概率p将某神经元权充值为0. 测试时所有神经元呈激活态，但权重需×(1-p)以保证训练测试的权重拥有相等期望。失活的神经元不参与训练，因此每次训练相当于面对一个全新网络——测试时相当于这些子网络的average ensemble；对于AlexNet等最后4096x4096的FC层：使用dropout=exponentially的子网络集成，泛化性效果显著提升。

工程使用时：直接将随机失活后的activation x （1/(1-p)), 则测试时不用再另作调整。（inverted dropout）

几乎所有配备FC的CBNN都会使用。在约束复杂度同时，也是一种针对深度模型的ensemble learning（高效集成学习）方法。

传统神经网络：单一神经元在backprop的梯度也受到其他神经元影响，所谓complex co-adaptation。dropout一定程度缓解了这个效应，降低了neuron的互相依赖，避免overfitting。

### 使用验证集

通常训练前从训练集中随机划分出一个子集——验证集，在训练阶段评测模型预测性能；一般每轮或每批后在该训练集和验证集上分别做前馈，预测训练集和验证集的样本标记，绘制学习曲线（准确率-epoch），来验证泛化能力。

若验证集acc一直低于训练集的acc，且无明显下降趋势——模型复杂度欠缺，表达能力有限，欠拟合! 需增加层数，调整activation增加非线性，减小正则化以提高复杂度；

若验证集acc一直低于训练集acc，且随epoch明显下降，则过拟合；增大正则化。

**early-stopping**：防止过拟合：可取验证集acc最高的那一轮结果作为最终网络，用于预测测试集；

此外还可通过数据手段——如增加训练数据，或数据扩充，在分类层加入随机噪声（隐式增加对模型的约束）来防止过拟合，提高泛化能力。



# 超参数设定和网络训练

### 超参数：

搭建架构前，需首先指定——输入图像像素/卷积层个数/卷积核参数等。

输入像素：便于GPU，一般压缩到2^n^大小，32，96，224，448，672（高分辨会提升性能，但计算消耗更多）。若直接改变输入图像分辨率，需重新改变FC输入滤波器大小或重新指定其他相关参数，以适配输入。

卷积层参数：

- 小卷积核 F
- padding P
- 步长 S

pooling 层  多为2x2 S2， 75%activation values将被丢弃。down-sampling。



### 训练技巧

**训练数据随机打乱**：因采用了mini-batch，可在每轮epoch将训练集打乱(shuffle)，这样不同epoch相同batch看到的数据不同；

**learning rate**：理想促进收敛，不理想导致loss爆炸；原则：

- 开始时不宜过大，0.01， 0.001为宜。如果没训练几个mini-batch，模型loss就急剧上升，说明lr过大，应减小，从头训练。
- 训练过程中，lr应随epoch减缓。一般有step decay，exponential decay，1/t decay。
- 借助模型learning curve。不妨保存每epoch后的loss，画出训练曲线（loss-epoch 图）：若loss刚开始几个批次就爆炸，则lr过大，应大幅减小重新训练；若一开始loss下降明显，但后劲不足，则换较小lr重新训练，或在后几轮改小lr重训练后几轮；若一直下降缓慢，应稍加大，继续观察，直到呈现理想学习率的曲线。
- 另外fine tune时有时也需特别关注lr。

**batch normalization 批规范化**：加快收敛，缓解梯度弥散，使训练更加容易且稳定；一定程度上提升泛化能力。

流程：每次SGD训练时，通过mini-batch来对activation做规范化操作，使其结果（输出的各个维度）的均值为0，方差为1；

1. 计算批处理的数据均值$\mu_b$,方差${\sigma_b}^2$；
2. 对这个batch的数据做规范化: $x_i$ --> $\widehat{x}_i$
3. 尺度偏移和变换：$\gamma \widehat{x}_i + \beta = BN(x_i)$ 
4. return 学习的参数 $\gamma ,\beta$.

作尺度变换和偏移操作，是为了使加入的BN还能够还原最初的输入，从而保证整个网络的capacity。 

*为何BN奏效？* 

- internal covariate shift：经典ML的假设：source domain和target domain的distribution一致，如果不一致，则是新的ML问题——如transfer learning或domain adaptation。协变量偏移即是分布不一致假设下的问题——条件概率一致，但是边缘概率不同。

BN的变种：feature normalization。常用于人脸识别中。作用于最后一层特征表示上，FN下一层为目标函数层，FN课提高习得特征的分辨能力。



### 优化算法

先假定w，学习率(步长)为$\eta$, 一阶梯度为g，第t轮训练。

**SGD**：每次批处理训练时计算误差，并反向传播，参数更新$w_t = w_{t-1}-\eta g$. 此时学习率可理解为当前批的梯度对网络整体参数更新的影响程度；最常见，收敛稳定，不过过慢。

**基于momentum的SGD**：改善SGD更新时可能的振荡，通过累计前几轮的动量辅助更新：$v_t = \mu v_{t-1} - \eta g, w_t=w_{t-1} + v_t$. $\mu$为动量因子，控制动量影响程度，多为静态的0.9，也可设为动态：如初始0.5，随epoch逐渐变成0.9/0.99. momentum 还可在loss后期陷在局部最优时帮助其跳出。

**Nesterov**在上述momentum方法更新梯度时加入对当前梯度的校正，在凸函数收敛上有更强的理论保证。 

$w_{ahead} = w_{t-1} + \mu v_{t-1} $, $v_t = \mu v_{t-1} - \eta \nabla w_{ahead}$, $w_t = w_{t-1} + v_t $. 

这些方法的lr却一直固定不变，没考虑lr的自适应性。

考虑lr自适应性的算法——

**Adagrad**：根据epoch不同对lr进行动态调整。需认为指定一个全局学习率$\mu_{global}$.且训练后期，分母上梯度累加使lr--> 0,导致训练过早结束。

**Adadelta**：加入衰减因子$\rho$ 消除adagrad对全局学习率的依赖。较大rho促进更新，[0,1]. 推荐$\rho = 0.95, \epsilon = 10^{-6}$.

**RMSProp**：Adadelta的特例，使用全局学习率替换s_t. 

**Adam**：带有动量的RMSProp。利用梯度一阶矩估计和二阶矩估计动态调整每个参数的learning rate。经过偏置校正后，每一次迭代学习率都有一个确定范围，使更新更平稳。仍寻指定基本学习率。

基于牛顿法的二阶优化方法：如LBFDGS，但涉及到计算海森矩阵，代价巨大。



### 微调 

微调已预训练好的网络模型——用目标任务数据在预训练模型上继续进行训练。需注意:

- 网络在source上收敛，因在target source上finetune时要用较小的lr(10^-4^或以下)

- 浅层更泛化，深层抽象对应高层语义。所以新数据上微调时泛化特征更新可能更小，故可根据不同层设置不同学习率，深层略大于浅层。

- 根据target和source的相似程度（怎么度量？见TF手册）采用不同的策略：

  - target data少，且于原始数据很相似:仅调靠近目标函数的后几层。
  - target data充足且相似：更多层，也可全部调。
  - 充足，差异较大：多调节一些，直至全部；
  - 极少，差异较大：比较麻烦，仍可尝试，未必成功。针对这种情况，还可借助部分原始数据和目标数据协同训练，可以在shallow feature space（浅层特征空间）选择目标数据的（nearest neighbor）作为原始数据子集。之后将微调阶段改造为multi-task learning：①目标任务基于原始数据子集；②目标任务基于全部目标数据。（Ge，Yu）

  ​

  ​

# 处理不平衡样本

imbalance：各类别样本数目不均衡，导致轻视“少数派”，泛化能力降低。

### 数据层面

多借助sampling使样本数趋于平衡。

**重采样** 包括上采样over-/up-sampling（对样本较少类，复制图像，或用数据扩充）和下采样under-/down-sampling（并非随机丢弃，而是在批处理训练时对每批抽取的图像严格控制其样本较多类别的图像数量）。仅用上采样容易过拟合，保险是结合使用。

**类别平衡采样**：将样本按类别分组，每个类别生成一个样本列表。训练时随机选择1/多个类别，然后在对应的列表中随机选择样本。保证每个类别参与训练的机会比较均衡。对于海量数据库比较繁琐。可以使用label shuffling(Shicai Yang)的平衡方法。**类别重组法**。

### 算法层面

对抗因不平衡产生的“欠学习”——增加小样本错误分数的惩罚，加入目标函数中（加强优化对小样本的注意力），此为cost-sensitive方法。

**cost-sensitive方法**：

1. 基于代价敏感矩阵C (KxK矩阵，一共有K类)，C(i, j)代表将i类错分为j类的惩罚 [0, $\infty$). 训练目标为：训练分类器g，使期望代价最小。
2. 基于代价敏感向量的代价敏感。对某样本(x_n, y_n)有一K维代价敏感向量c_n，第k维表示错分为第k类的惩罚。
3. 错分惩罚/错分权重指定方式：
   1. 按样本比例指定：样本大的类别的样本被错分到其他类时，惩罚权重较小；反之权重应加大。
   2. 按混淆矩阵指定：confusion matrix，一种算法分析工具，度量算法在监督学习中预测能力的优劣，又称列联表/误差矩阵。每一列代表一个类的实例预测，每一行表示其真实类别。（即表达第i类的样本有多少个被分到了第j类。）



# 模型集成方法

训练多个学习器再组合使用。

### 数据层面

**测试阶段数据扩充**：如multi scale/random crop等（如random crop得n张图，测试时需要用训练好的深度网络对n张图分别预测，再将预测的各类置信度平均作为最终预测结果）

**easy ensemble**：针对不平衡样本,对多数派采取down-sampling，每次采样数依照数目最少的类别而定，然后针对每次采样得到的子集训练，反复采样+训练几次，最后测试时依据训练得的n个模型的结果取平均/投票。



### 模型层面

#### 单模型集成

**多层特征融合** multi-layer ensemble. cnn具有层次性特点，不同层特征富含的语义信息互补，故使用多层特征融合策略，一般直接将不同层网络特征级联（concatenate），应选取：最好是靠近目标函数的几层（高层语义性/分辨能力更强）。

**网络快照集成法 snapshot ensemble**：非常多局部最优——通过循环调整lr（cyclic learning rate schedule，lr随iteration——即mini-batch批处理训练次数/随机梯度下降次数——改变）使网络依次收敛到不同的局部最优解（即lr慢慢减缓，得一局部最优，再重新放大，下一循环训练……循环余弦退货cyclic cosine annealing）M个循环得M个不同收敛状态的模型，施以集成策略。

#### 多模型集成

**多模型生成策略**

- 同模型不同初始。
- 同模型不同epoch数。（epoch ensemble/epoch fusion）
- 不同目标函数。然后在预测时可对不同模型结果做score level的平均/投票，或做特征级别feature level的模型集成（不同网络得到的深度特征抽出后级联作为最终特征，然后离线训练浅层分类器）
- 不同网络结构。

**多模型集成方法**

- simple averaging：不同模型产生的类别置信度直接加以平均
- weighted averaging：权重调节不同模型输出的重要程度
- voting，majority voting with rejection option；plurality voting一定会返回某个类别。
- stacking：二次集成法。在一阶学习过程的输出上作second-level learning/meta learning。容易过拟合。

dropout也是一种集成方法。



# 数学

**norm**：

- 1-norm：向量元素绝对值的sum（l1-loss）
- 2-norm：euclid norm，向量长度（l2-loss）
- $\infty$-norm: 所有向量元素绝对值的最大值。
- p-norm：1，2为p的特殊形式（p次方+1/p次幂）
- 矩阵：
- 列范数：A的每列绝对值之和的最大值。
- 行范数：A的每行绝对值之和的最大值。
- 2-范数
- F-范数
- p-范数



